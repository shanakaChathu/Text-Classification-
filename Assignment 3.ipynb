{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "2y_62vYqEYAe",
    "outputId": "94626bc5-4e13-49b3-c27d-3fa08c8a70bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shanaka_105298\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\shanaka_105298\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shanaka_105298\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shanaka_105298\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Shanaka_105298\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\Shanaka_105298\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "C:\\Users\\Shanaka_105298\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import string\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "nltk.download('nps_chat')\n",
    "import collections \n",
    "from nltk import precision\n",
    "from nltk.metrics.scores import (precision, recall)\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "from nltk.classify import MaxentClassifier, accuracy\n",
    "import re \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import random\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MulmKFKQ_5A"
   },
   "outputs": [],
   "source": [
    "#Functions to calculate accuracy scores in each fold \n",
    "def precisionScore(test_set,classList,classifier):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    pList=[]\n",
    "  \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    \n",
    "    for x in range(len(classList)):\n",
    "        p=nltk.precision(refsets[classList[x]], testsets[classList[x]])\n",
    "        pList.append(p)\n",
    "  \n",
    "    pList=[0 if v is None else v for v in pList]\n",
    "    return pList\n",
    "\n",
    "\n",
    "def recallScore(test_set,classList,classifier):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    rList=[]\n",
    "  \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    \n",
    "    for x in range(len(classList)):\n",
    "        r=nltk.recall(refsets[classList[x]], testsets[classList[x]])\n",
    "        rList.append(r)\n",
    "  \n",
    "    rList=[0 if v is None else v for v in rList]\n",
    "    return rList\n",
    "\n",
    "def f1Score(test_set,classList,classifier):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    pList=[]\n",
    "    rList=[]\n",
    "  \n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    \n",
    "    for x in range(len(classList)):\n",
    "        p=nltk.precision(refsets[classList[x]], testsets[classList[x]])\n",
    "        r=nltk.recall(refsets[classList[x]], testsets[classList[x]])\n",
    "\n",
    "        pList.append(p)\n",
    "        rList.append(r)\n",
    "\n",
    "    pList=[0 if v is None else v for v in pList]\n",
    "    rList=[0 if v is None else v for v in rList]\n",
    "\n",
    "    df=pd.DataFrame({\"p\":pList,\"r\":rList})\n",
    "    df['f1']=(2*df['p']*df['r'])/(df['p']+df['r'])\n",
    "    f1List=list(df.f1)\n",
    "    f1List = [0 if math.isnan(x) else x for x in f1List]\n",
    "    return f1List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvBhbFOiXM8S"
   },
   "outputs": [],
   "source": [
    "#Functions of models \n",
    "def naiveBayes(train_set):\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    return classifier \n",
    "\n",
    "def decisionTree(train_set):\n",
    "    classifier=nltk.DecisionTreeClassifier.train(train_set)\n",
    "    return classifier\n",
    "\n",
    "def maxNet(train_set):\n",
    "    classifier = MaxentClassifier.train(train_set)\n",
    "    return classifier\n",
    "\n",
    "def model(featureSets,classList,model):\n",
    "    featureArray=np.asarray(featuresets)\n",
    "    kfold = StratifiedKFold(10, True, 1)\n",
    "    #kfold = KFold(10, True, 1)\n",
    "    dfPre=pd.DataFrame({\"className\":classList})\n",
    "    dfRec=pd.DataFrame({\"className\":classList})\n",
    "    dfF1=pd.DataFrame({\"className\":classList})\n",
    "    accList=[]\n",
    "    i=1\n",
    "    for train, test in kfold.split(featureArray,featureArray[:,1]):\n",
    "    #for train, test in kfold.split(featureArray):\n",
    "        train_set, test_set = featureArray[train], featureArray[test]\n",
    "        if(model=='NB'):\n",
    "            classifier=naiveBayes(train_set)\n",
    "        elif(model=='MXN'):\n",
    "            classifier=maxNet(train_set) \n",
    "        elif(model=='DT'):\n",
    "            classifier=decisionTree(train_set) \n",
    "        acc=nltk.classify.accuracy(classifier, test_set)\n",
    "        accList.append(acc)\n",
    "        pre=precisionScore(test_set,classList,classifier)\n",
    "        rec=recallScore(test_set,classList,classifier)\n",
    "        f1=f1Score(test_set,classList,classifier)\n",
    "        var='k'+str(i)\n",
    "        dfPre[var]=pre\n",
    "        dfRec[var]=rec\n",
    "        dfF1[var]=f1\n",
    "        i=i+1\n",
    "        \n",
    "    print(\"Overall accuracy of the model\",np.mean(accList))\n",
    "    dfPre['avgPrecision']=(dfPre['k1']+dfPre['k2']+dfPre['k3']+dfPre['k4']+dfPre['k5']+dfPre['k6']+dfPre['k7']+dfPre['k8']+dfPre['k9']+dfPre['k10'])/10\n",
    "    dfRec['avgRecall']=(dfRec['k1']+dfRec['k2']+dfRec['k3']+dfRec['k4']+dfRec['k5']+dfRec['k6']+dfRec['k7']+dfRec['k8']+dfRec['k9']+dfRec['k10'])/10\n",
    "    dfF1['avgF1']=(dfF1['k1']+dfF1['k2']+dfF1['k3']+dfF1['k4']+dfF1['k5']+dfF1['k6']+dfF1['k7']+dfF1['k8']+dfF1['k9']+dfF1['k10'])/10\n",
    "\n",
    "    dfPre=dfPre[['className','avgPrecision']]\n",
    "    dfRec=dfRec[['className','avgRecall']]\n",
    "    dfF1=dfF1[['className','avgF1']]\n",
    "\n",
    "    return dfPre,dfRec,dfF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10567, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>now im left with this gay name</td>\n",
       "      <td>Statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>:P</td>\n",
       "      <td>Emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             post class_name\n",
       "0  now im left with this gay name  Statement\n",
       "1                              :P    Emotion"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the NPS chat by calling NLTK API\n",
    "posts = nltk.corpus.nps_chat.xml_posts()\n",
    "#Class Lists \n",
    "postList=[]\n",
    "classList=[]\n",
    "for post in posts:\n",
    "    postList.append(post.text)\n",
    "    classList.append(post.get('class'))\n",
    "df=pd.DataFrame({'post':postList,'class_name':classList})\n",
    "print(df.shape)\n",
    "classList=list(df.class_name.unique())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of all caps words in sentence 65\n",
      "Maximum number of '!' in sentence 54\n"
     ]
    }
   ],
   "source": [
    "#Calclulating valuess related to the normalizations \n",
    "def allCaps(x):\n",
    "    count=0\n",
    "    nltk_tokens = nltk.word_tokenize(x.post)  \n",
    "    for word in nltk_tokens:\n",
    "        if(word.isupper()):\n",
    "            count=count+1\n",
    "    return count\n",
    "\n",
    "def nVish(x):\n",
    "    count=0\n",
    "    nltk_tokens=nltk.word_tokenize(x.post)  \n",
    "    for word in nltk_tokens:\n",
    "        if('!' in word):\n",
    "            count=count+1\n",
    "    return count\n",
    "\n",
    "df['nAllCaps']=df.apply(allCaps,axis=1)\n",
    "df['nVish']=df.apply(nVish,axis=1)\n",
    "print(\"Maximum number of all caps words in sentence\",max(df.nAllCaps))\n",
    "print(\"Maximum number of '!' in sentence\",max(df.nVish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rM94jnk_9V4D"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering functions \n",
    "def textClean(nltk_tokens):\n",
    "    nltk_tokens=[word.lower() for word in nltk_tokens]\n",
    "    #nltk_tokens = [word for word in nltk_tokens if word.isalpha()]\n",
    "    #nltk_tokens = [w for w in nltk_tokens if not w in stop_words]\n",
    "    nltk_tokens=[lemmatizer.lemmatize(word) for word in nltk_tokens]\n",
    "    return nltk_tokens\n",
    "\n",
    "\n",
    "def firstTokenYN(post):\n",
    "    nltk_tokens = nltk.word_tokenize(post)  \n",
    "    nltk_tokens=[word.lower() for word in nltk_tokens]\n",
    "    dic=['is','am','are','does','did','do','was','were','have','should','can','could','has']\n",
    "    fword=nltk_tokens[0]\n",
    "    if fword in dic:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0\n",
    "      \n",
    "def firstTokenWH(post):\n",
    "    nltk_tokens = nltk.word_tokenize(post)  \n",
    "    nltk_tokens=[word.lower() for word in nltk_tokens]\n",
    "    dic=['what','who','whose','which','when','how','whom','where','why','whats','whoever','whomever','wherever']\n",
    "    fword=nltk_tokens[0]\n",
    "    if fword in dic:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def firstTokenConj(post):\n",
    "    nltk_tokens = nltk.word_tokenize(post)  \n",
    "    nltk_tokens=[word.lower() for word in nltk_tokens]\n",
    "    a=nltk.pos_tag(nltk_tokens)\n",
    "    if(a[0][1]=='CC'):\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def allCaps(post):\n",
    "    count=0\n",
    "    nltk_tokens = nltk.word_tokenize(post)  \n",
    "    for word in nltk_tokens:\n",
    "        if(word.isupper()):\n",
    "            count=count+1\n",
    "    return count\n",
    "\n",
    "def nVish(post):\n",
    "    count=0\n",
    "    nltk_tokens=nltk.word_tokenize(post)  \n",
    "    for word in nltk_tokens:\n",
    "        if('!' in word):\n",
    "            count=count+1\n",
    "    return count\n",
    "    \n",
    "def feature(post):\n",
    "    features = {}\n",
    "    nltk_tokens = nltk.word_tokenize(post)  \n",
    "    nltk_tokens=textClean(nltk_tokens)\n",
    "    l1=list(nltk.bigrams(nltk_tokens))\n",
    "    l2=list(nltk.trigrams(nltk_tokens)) \n",
    "    for word in nltk_tokens:\n",
    "        features['contains({})'.format(word)] = True    #f1\n",
    "    #for x in range(len(l1)):\n",
    "    #    features['contains({})'.format(l1[x])] = True\n",
    "    #for x in range(len(l2)):\n",
    "    #    features['contains({})'.format(l2[x])] = True\n",
    "    features['nwords']=len(nltk_tokens) #f2\n",
    "    features['firstTokenYN']=firstTokenYN(post) #f3\n",
    "    features['firstToeknWH']=firstTokenWH(post) #f4\n",
    "    features['nAllCaps']=allCaps(post)/65 #f5\n",
    "    features['firstTokenConj']=firstTokenConj(post) #f6\n",
    "    features['nvish']=nVish(post)/54 #f7\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the NPS chat by calling NLTK API\n",
    "posts = nltk.corpus.nps_chat.xml_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zEOVlLp0EYFl",
    "outputId": "1ad9140d-a054-4163-eca5-da0afd946594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model 0.6732240366143137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>className</th>\n",
       "      <th>avgPrecision</th>\n",
       "      <th>avgRecall</th>\n",
       "      <th>avgF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Statement</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.470343</td>\n",
       "      <td>0.579938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Emotion</td>\n",
       "      <td>0.732919</td>\n",
       "      <td>0.779287</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>System</td>\n",
       "      <td>0.989909</td>\n",
       "      <td>0.856758</td>\n",
       "      <td>0.918468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Greet</td>\n",
       "      <td>0.892376</td>\n",
       "      <td>0.851106</td>\n",
       "      <td>0.870663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Accept</td>\n",
       "      <td>0.463579</td>\n",
       "      <td>0.335145</td>\n",
       "      <td>0.385706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Reject</td>\n",
       "      <td>0.150863</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.193538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>whQuestion</td>\n",
       "      <td>0.798721</td>\n",
       "      <td>0.761705</td>\n",
       "      <td>0.778316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Continuer</td>\n",
       "      <td>0.468902</td>\n",
       "      <td>0.226103</td>\n",
       "      <td>0.294615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ynQuestion</td>\n",
       "      <td>0.588143</td>\n",
       "      <td>0.736364</td>\n",
       "      <td>0.652421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>yAnswer</td>\n",
       "      <td>0.314740</td>\n",
       "      <td>0.279091</td>\n",
       "      <td>0.293929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bye</td>\n",
       "      <td>0.796111</td>\n",
       "      <td>0.651842</td>\n",
       "      <td>0.709047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Clarify</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.016571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Emphasis</td>\n",
       "      <td>0.340780</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.470266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>nAnswer</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.569643</td>\n",
       "      <td>0.177606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.043105</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.078564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     className  avgPrecision  avgRecall     avgF1\n",
       "0    Statement      0.757407   0.470343  0.579938\n",
       "1      Emotion      0.732919   0.779287  0.755177\n",
       "2       System      0.989909   0.856758  0.918468\n",
       "3        Greet      0.892376   0.851106  0.870663\n",
       "4       Accept      0.463579   0.335145  0.385706\n",
       "5       Reject      0.150863   0.290000  0.193538\n",
       "6   whQuestion      0.798721   0.761705  0.778316\n",
       "7    Continuer      0.468902   0.226103  0.294615\n",
       "8   ynQuestion      0.588143   0.736364  0.652421\n",
       "9      yAnswer      0.314740   0.279091  0.293929\n",
       "10         Bye      0.796111   0.651842  0.709047\n",
       "11     Clarify      0.008775   0.150000  0.016571\n",
       "12    Emphasis      0.340780   0.773684  0.470266\n",
       "13     nAnswer      0.105700   0.569643  0.177606\n",
       "14       Other      0.043105   0.466667  0.078564"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Naivebayess Model\n",
    "#featuresets = [(feature(post.text), post.get('class'))  for post in posts]\n",
    "random.shuffle(featuresets)\n",
    "dfPre,dfRec,dfF1=model(featuresets,classList,'NB')\n",
    "\n",
    "df=pd.merge(dfPre,dfRec,on='className',how='inner')\n",
    "df=pd.merge(df,dfF1,on='className',how='inner')\n",
    "\n",
    "df.to_csv(\"dfNaiveBayes.csv\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19227        0.650\n",
      "             3          -0.88407        0.754\n",
      "             4          -0.73017        0.805\n",
      "             5          -0.63231        0.834\n",
      "             6          -0.56376        0.852\n",
      "             7          -0.51262        0.865\n",
      "             8          -0.47268        0.879\n",
      "             9          -0.44039        0.887\n",
      "            10          -0.41357        0.896\n",
      "            11          -0.39083        0.904\n",
      "            12          -0.37121        0.910\n",
      "            13          -0.35406        0.916\n",
      "            14          -0.33890        0.921\n",
      "            15          -0.32536        0.924\n",
      "            16          -0.31318        0.930\n",
      "            17          -0.30215        0.933\n",
      "            18          -0.29209        0.937\n",
      "            19          -0.28288        0.939\n",
      "            20          -0.27440        0.941\n",
      "            21          -0.26656        0.944\n",
      "            22          -0.25929        0.946\n",
      "            23          -0.25252        0.947\n",
      "            24          -0.24620        0.949\n",
      "            25          -0.24028        0.950\n",
      "            26          -0.23473        0.953\n",
      "            27          -0.22950        0.954\n",
      "            28          -0.22457        0.955\n",
      "            29          -0.21992        0.955\n",
      "            30          -0.21551        0.956\n",
      "            31          -0.21133        0.957\n",
      "            32          -0.20736        0.958\n",
      "            33          -0.20358        0.959\n",
      "            34          -0.19998        0.959\n",
      "            35          -0.19655        0.960\n",
      "            36          -0.19327        0.961\n",
      "            37          -0.19013        0.962\n",
      "            38          -0.18713        0.962\n",
      "            39          -0.18425        0.963\n",
      "            40          -0.18149        0.963\n",
      "            41          -0.17883        0.964\n",
      "            42          -0.17628        0.965\n",
      "            43          -0.17383        0.966\n",
      "            44          -0.17146        0.967\n",
      "            45          -0.16918        0.967\n",
      "            46          -0.16698        0.967\n",
      "            47          -0.16486        0.967\n",
      "            48          -0.16281        0.968\n",
      "            49          -0.16083        0.968\n",
      "            50          -0.15891        0.968\n",
      "            51          -0.15705        0.969\n",
      "            52          -0.15525        0.969\n",
      "            53          -0.15351        0.969\n",
      "            54          -0.15182        0.970\n",
      "            55          -0.15018        0.970\n",
      "            56          -0.14859        0.970\n",
      "            57          -0.14704        0.970\n",
      "            58          -0.14554        0.970\n",
      "            59          -0.14407        0.971\n",
      "            60          -0.14265        0.971\n",
      "            61          -0.14127        0.971\n",
      "            62          -0.13992        0.971\n",
      "            63          -0.13861        0.972\n",
      "            64          -0.13733        0.972\n",
      "            65          -0.13608        0.972\n",
      "            66          -0.13487        0.972\n",
      "            67          -0.13368        0.972\n",
      "            68          -0.13253        0.973\n",
      "            69          -0.13140        0.973\n",
      "            70          -0.13029        0.973\n",
      "            71          -0.12921        0.973\n",
      "            72          -0.12816        0.973\n",
      "            73          -0.12713        0.973\n",
      "            74          -0.12612        0.973\n",
      "            75          -0.12514        0.973\n",
      "            76          -0.12418        0.973\n",
      "            77          -0.12323        0.974\n",
      "            78          -0.12231        0.974\n",
      "            79          -0.12140        0.974\n",
      "            80          -0.12052        0.974\n",
      "            81          -0.11965        0.974\n",
      "            82          -0.11880        0.975\n",
      "            83          -0.11797        0.975\n",
      "            84          -0.11715        0.975\n",
      "            85          -0.11635        0.975\n",
      "            86          -0.11556        0.976\n",
      "            87          -0.11479        0.976\n",
      "            88          -0.11403        0.976\n",
      "            89          -0.11329        0.976\n",
      "            90          -0.11256        0.977\n",
      "            91          -0.11185        0.977\n",
      "            92          -0.11114        0.977\n",
      "            93          -0.11045        0.977\n",
      "            94          -0.10977        0.977\n",
      "            95          -0.10911        0.977\n",
      "            96          -0.10845        0.977\n",
      "            97          -0.10781        0.977\n",
      "            98          -0.10717        0.977\n",
      "            99          -0.10655        0.977\n",
      "         Final          -0.10594        0.977\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19341        0.650\n",
      "             3          -0.88622        0.756\n",
      "             4          -0.73280        0.800\n",
      "             5          -0.63497        0.832\n",
      "             6          -0.56627        0.853\n",
      "             7          -0.51495        0.864\n",
      "             8          -0.47482        0.877\n",
      "             9          -0.44235        0.887\n",
      "            10          -0.41537        0.896\n",
      "            11          -0.39248        0.904\n",
      "            12          -0.37274        0.908\n",
      "            13          -0.35548        0.914\n",
      "            14          -0.34022        0.918\n",
      "            15          -0.32659        0.924\n",
      "            16          -0.31434        0.929\n",
      "            17          -0.30323        0.933\n",
      "            18          -0.29311        0.936\n",
      "            19          -0.28384        0.940\n",
      "            20          -0.27530        0.942\n",
      "            21          -0.26741        0.944\n",
      "            22          -0.26009        0.946\n",
      "            23          -0.25328        0.948\n",
      "            24          -0.24692        0.949\n",
      "            25          -0.24097        0.952\n",
      "            26          -0.23538        0.953\n",
      "            27          -0.23012        0.954\n",
      "            28          -0.22515        0.955\n",
      "            29          -0.22047        0.956\n",
      "            30          -0.21603        0.956\n",
      "            31          -0.21182        0.957\n",
      "            32          -0.20782        0.958\n",
      "            33          -0.20402        0.958\n",
      "            34          -0.20039        0.959\n",
      "            35          -0.19693        0.960\n",
      "            36          -0.19363        0.960\n",
      "            37          -0.19047        0.962\n",
      "            38          -0.18744        0.962\n",
      "            39          -0.18454        0.963\n",
      "            40          -0.18176        0.964\n",
      "            41          -0.17908        0.964\n",
      "            42          -0.17651        0.965\n",
      "            43          -0.17403        0.965\n",
      "            44          -0.17165        0.965\n",
      "            45          -0.16935        0.966\n",
      "            46          -0.16713        0.966\n",
      "            47          -0.16499        0.967\n",
      "            48          -0.16292        0.967\n",
      "            49          -0.16092        0.968\n",
      "            50          -0.15898        0.968\n",
      "            51          -0.15711        0.969\n",
      "            52          -0.15529        0.969\n",
      "            53          -0.15353        0.969\n",
      "            54          -0.15182        0.970\n",
      "            55          -0.15017        0.970\n",
      "            56          -0.14856        0.970\n",
      "            57          -0.14700        0.971\n",
      "            58          -0.14548        0.971\n",
      "            59          -0.14400        0.972\n",
      "            60          -0.14256        0.972\n",
      "            61          -0.14116        0.972\n",
      "            62          -0.13980        0.972\n",
      "            63          -0.13848        0.973\n",
      "            64          -0.13718        0.973\n",
      "            65          -0.13592        0.973\n",
      "            66          -0.13469        0.973\n",
      "            67          -0.13349        0.973\n",
      "            68          -0.13232        0.973\n",
      "            69          -0.13118        0.974\n",
      "            70          -0.13006        0.974\n",
      "            71          -0.12897        0.974\n",
      "            72          -0.12791        0.974\n",
      "            73          -0.12686        0.974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            74          -0.12584        0.974\n",
      "            75          -0.12485        0.975\n",
      "            76          -0.12387        0.975\n",
      "            77          -0.12292        0.975\n",
      "            78          -0.12198        0.975\n",
      "            79          -0.12107        0.975\n",
      "            80          -0.12017        0.975\n",
      "            81          -0.11929        0.975\n",
      "            82          -0.11843        0.975\n",
      "            83          -0.11758        0.975\n",
      "            84          -0.11676        0.975\n",
      "            85          -0.11594        0.975\n",
      "            86          -0.11515        0.975\n",
      "            87          -0.11436        0.976\n",
      "            88          -0.11360        0.976\n",
      "            89          -0.11284        0.976\n",
      "            90          -0.11210        0.976\n",
      "            91          -0.11138        0.976\n",
      "            92          -0.11066        0.976\n",
      "            93          -0.10996        0.976\n",
      "            94          -0.10928        0.977\n",
      "            95          -0.10860        0.977\n",
      "            96          -0.10793        0.977\n",
      "            97          -0.10728        0.977\n",
      "            98          -0.10664        0.977\n",
      "            99          -0.10600        0.977\n",
      "         Final          -0.10538        0.977\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19709        0.651\n",
      "             3          -0.88774        0.755\n",
      "             4          -0.73307        0.803\n",
      "             5          -0.63457        0.832\n",
      "             6          -0.56557        0.851\n",
      "             7          -0.51413        0.864\n",
      "             8          -0.47400        0.877\n",
      "             9          -0.44158        0.886\n",
      "            10          -0.41468        0.895\n",
      "            11          -0.39187        0.903\n",
      "            12          -0.37220        0.909\n",
      "            13          -0.35502        0.913\n",
      "            14          -0.33982        0.917\n",
      "            15          -0.32625        0.924\n",
      "            16          -0.31405        0.927\n",
      "            17          -0.30299        0.931\n",
      "            18          -0.29291        0.935\n",
      "            19          -0.28367        0.937\n",
      "            20          -0.27516        0.940\n",
      "            21          -0.26730        0.942\n",
      "            22          -0.26000        0.944\n",
      "            23          -0.25320        0.945\n",
      "            24          -0.24686        0.947\n",
      "            25          -0.24091        0.948\n",
      "            26          -0.23533        0.950\n",
      "            27          -0.23008        0.952\n",
      "            28          -0.22512        0.953\n",
      "            29          -0.22044        0.955\n",
      "            30          -0.21601        0.956\n",
      "            31          -0.21180        0.957\n",
      "            32          -0.20780        0.958\n",
      "            33          -0.20400        0.959\n",
      "            34          -0.20038        0.959\n",
      "            35          -0.19692        0.960\n",
      "            36          -0.19362        0.961\n",
      "            37          -0.19046        0.961\n",
      "            38          -0.18743        0.962\n",
      "            39          -0.18453        0.962\n",
      "            40          -0.18174        0.964\n",
      "            41          -0.17907        0.965\n",
      "            42          -0.17650        0.965\n",
      "            43          -0.17402        0.965\n",
      "            44          -0.17163        0.966\n",
      "            45          -0.16934        0.967\n",
      "            46          -0.16712        0.967\n",
      "            47          -0.16497        0.967\n",
      "            48          -0.16291        0.967\n",
      "            49          -0.16090        0.968\n",
      "            50          -0.15897        0.969\n",
      "            51          -0.15709        0.969\n",
      "            52          -0.15528        0.969\n",
      "            53          -0.15352        0.970\n",
      "            54          -0.15181        0.970\n",
      "            55          -0.15015        0.971\n",
      "            56          -0.14854        0.971\n",
      "            57          -0.14698        0.971\n",
      "            58          -0.14546        0.971\n",
      "            59          -0.14399        0.971\n",
      "            60          -0.14255        0.972\n",
      "            61          -0.14115        0.972\n",
      "            62          -0.13979        0.972\n",
      "            63          -0.13846        0.973\n",
      "            64          -0.13717        0.973\n",
      "            65          -0.13591        0.973\n",
      "            66          -0.13468        0.973\n",
      "            67          -0.13348        0.973\n",
      "            68          -0.13231        0.973\n",
      "            69          -0.13117        0.974\n",
      "            70          -0.13005        0.974\n",
      "            71          -0.12896        0.974\n",
      "            72          -0.12790        0.974\n",
      "            73          -0.12685        0.974\n",
      "            74          -0.12583        0.974\n",
      "            75          -0.12484        0.974\n",
      "            76          -0.12386        0.975\n",
      "            77          -0.12291        0.975\n",
      "            78          -0.12197        0.975\n",
      "            79          -0.12106        0.975\n",
      "            80          -0.12016        0.975\n",
      "            81          -0.11928        0.976\n",
      "            82          -0.11842        0.976\n",
      "            83          -0.11758        0.976\n",
      "            84          -0.11675        0.976\n",
      "            85          -0.11594        0.976\n",
      "            86          -0.11514        0.977\n",
      "            87          -0.11436        0.977\n",
      "            88          -0.11360        0.977\n",
      "            89          -0.11284        0.977\n",
      "            90          -0.11211        0.977\n",
      "            91          -0.11138        0.977\n",
      "            92          -0.11067        0.977\n",
      "            93          -0.10997        0.977\n",
      "            94          -0.10928        0.977\n",
      "            95          -0.10860        0.977\n",
      "            96          -0.10794        0.978\n",
      "            97          -0.10729        0.978\n",
      "            98          -0.10664        0.978\n",
      "            99          -0.10601        0.978\n",
      "         Final          -0.10539        0.978\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19613        0.650\n",
      "             3          -0.88728        0.757\n",
      "             4          -0.73290        0.803\n",
      "             5          -0.63462        0.831\n",
      "             6          -0.56572        0.851\n",
      "             7          -0.51431        0.864\n",
      "             8          -0.47417        0.876\n",
      "             9          -0.44172        0.885\n",
      "            10          -0.41479        0.896\n",
      "            11          -0.39195        0.903\n",
      "            12          -0.37227        0.910\n",
      "            13          -0.35506        0.914\n",
      "            14          -0.33985        0.918\n",
      "            15          -0.32627        0.924\n",
      "            16          -0.31406        0.926\n",
      "            17          -0.30299        0.931\n",
      "            18          -0.29290        0.935\n",
      "            19          -0.28366        0.939\n",
      "            20          -0.27516        0.942\n",
      "            21          -0.26729        0.944\n",
      "            22          -0.25999        0.946\n",
      "            23          -0.25320        0.948\n",
      "            24          -0.24685        0.949\n",
      "            25          -0.24091        0.951\n",
      "            26          -0.23533        0.952\n",
      "            27          -0.23007        0.953\n",
      "            28          -0.22512        0.954\n",
      "            29          -0.22043        0.956\n",
      "            30          -0.21600        0.958\n",
      "            31          -0.21179        0.959\n",
      "            32          -0.20779        0.960\n",
      "            33          -0.20399        0.961\n",
      "            34          -0.20036        0.963\n",
      "            35          -0.19690        0.963\n",
      "            36          -0.19359        0.964\n",
      "            37          -0.19043        0.964\n",
      "            38          -0.18740        0.965\n",
      "            39          -0.18449        0.966\n",
      "            40          -0.18171        0.966\n",
      "            41          -0.17903        0.967\n",
      "            42          -0.17645        0.967\n",
      "            43          -0.17397        0.967\n",
      "            44          -0.17158        0.967\n",
      "            45          -0.16928        0.968\n",
      "            46          -0.16706        0.968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            47          -0.16491        0.969\n",
      "            48          -0.16284        0.969\n",
      "            49          -0.16083        0.970\n",
      "            50          -0.15889        0.970\n",
      "            51          -0.15702        0.970\n",
      "            52          -0.15520        0.971\n",
      "            53          -0.15343        0.971\n",
      "            54          -0.15172        0.971\n",
      "            55          -0.15006        0.972\n",
      "            56          -0.14845        0.972\n",
      "            57          -0.14689        0.972\n",
      "            58          -0.14536        0.972\n",
      "            59          -0.14389        0.972\n",
      "            60          -0.14245        0.973\n",
      "            61          -0.14105        0.973\n",
      "            62          -0.13968        0.973\n",
      "            63          -0.13836        0.973\n",
      "            64          -0.13706        0.974\n",
      "            65          -0.13580        0.974\n",
      "            66          -0.13457        0.974\n",
      "            67          -0.13337        0.974\n",
      "            68          -0.13220        0.975\n",
      "            69          -0.13106        0.975\n",
      "            70          -0.12994        0.975\n",
      "            71          -0.12885        0.975\n",
      "            72          -0.12779        0.975\n",
      "            73          -0.12674        0.975\n",
      "            74          -0.12573        0.976\n",
      "            75          -0.12473        0.976\n",
      "            76          -0.12376        0.976\n",
      "            77          -0.12280        0.976\n",
      "            78          -0.12187        0.976\n",
      "            79          -0.12095        0.976\n",
      "            80          -0.12006        0.977\n",
      "            81          -0.11918        0.977\n",
      "            82          -0.11832        0.977\n",
      "            83          -0.11748        0.977\n",
      "            84          -0.11665        0.977\n",
      "            85          -0.11585        0.977\n",
      "            86          -0.11505        0.978\n",
      "            87          -0.11427        0.978\n",
      "            88          -0.11351        0.978\n",
      "            89          -0.11276        0.978\n",
      "            90          -0.11202        0.978\n",
      "            91          -0.11130        0.978\n",
      "            92          -0.11059        0.978\n",
      "            93          -0.10989        0.978\n",
      "            94          -0.10920        0.978\n",
      "            95          -0.10853        0.979\n",
      "            96          -0.10787        0.979\n",
      "            97          -0.10722        0.979\n",
      "            98          -0.10658        0.979\n",
      "            99          -0.10595        0.979\n",
      "         Final          -0.10533        0.979\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19614        0.650\n",
      "             3          -0.89333        0.751\n",
      "             4          -0.74149        0.801\n",
      "             5          -0.64416        0.830\n",
      "             6          -0.57543        0.851\n",
      "             7          -0.52382        0.864\n",
      "             8          -0.48331        0.878\n",
      "             9          -0.45045        0.887\n",
      "            10          -0.42308        0.897\n",
      "            11          -0.39983        0.904\n",
      "            12          -0.37976        0.911\n",
      "            13          -0.36219        0.916\n",
      "            14          -0.34665        0.921\n",
      "            15          -0.33278        0.925\n",
      "            16          -0.32029        0.928\n",
      "            17          -0.30897        0.931\n",
      "            18          -0.29866        0.934\n",
      "            19          -0.28921        0.938\n",
      "            20          -0.28051        0.940\n",
      "            21          -0.27247        0.942\n",
      "            22          -0.26501        0.947\n",
      "            23          -0.25806        0.948\n",
      "            24          -0.25157        0.950\n",
      "            25          -0.24550        0.951\n",
      "            26          -0.23980        0.952\n",
      "            27          -0.23443        0.954\n",
      "            28          -0.22937        0.955\n",
      "            29          -0.22459        0.956\n",
      "            30          -0.22006        0.956\n",
      "            31          -0.21577        0.957\n",
      "            32          -0.21169        0.958\n",
      "            33          -0.20780        0.959\n",
      "            34          -0.20410        0.960\n",
      "            35          -0.20057        0.960\n",
      "            36          -0.19720        0.961\n",
      "            37          -0.19398        0.963\n",
      "            38          -0.19089        0.963\n",
      "            39          -0.18793        0.964\n",
      "            40          -0.18508        0.964\n",
      "            41          -0.18235        0.965\n",
      "            42          -0.17973        0.966\n",
      "            43          -0.17720        0.966\n",
      "            44          -0.17477        0.967\n",
      "            45          -0.17242        0.967\n",
      "            46          -0.17016        0.967\n",
      "            47          -0.16797        0.967\n",
      "            48          -0.16586        0.968\n",
      "            49          -0.16382        0.968\n",
      "            50          -0.16184        0.969\n",
      "            51          -0.15993        0.969\n",
      "            52          -0.15808        0.970\n",
      "            53          -0.15628        0.970\n",
      "            54          -0.15454        0.970\n",
      "            55          -0.15285        0.970\n",
      "            56          -0.15120        0.971\n",
      "            57          -0.14961        0.971\n",
      "            58          -0.14806        0.971\n",
      "            59          -0.14655        0.971\n",
      "            60          -0.14509        0.971\n",
      "            61          -0.14366        0.971\n",
      "            62          -0.14227        0.972\n",
      "            63          -0.14092        0.972\n",
      "            64          -0.13960        0.972\n",
      "            65          -0.13831        0.972\n",
      "            66          -0.13706        0.972\n",
      "            67          -0.13583        0.972\n",
      "            68          -0.13464        0.972\n",
      "            69          -0.13347        0.973\n",
      "            70          -0.13234        0.973\n",
      "            71          -0.13122        0.973\n",
      "            72          -0.13014        0.973\n",
      "            73          -0.12907        0.973\n",
      "            74          -0.12803        0.974\n",
      "            75          -0.12702        0.974\n",
      "            76          -0.12602        0.974\n",
      "            77          -0.12505        0.974\n",
      "            78          -0.12410        0.975\n",
      "            79          -0.12316        0.975\n",
      "            80          -0.12225        0.975\n",
      "            81          -0.12135        0.975\n",
      "            82          -0.12048        0.975\n",
      "            83          -0.11962        0.976\n",
      "            84          -0.11877        0.976\n",
      "            85          -0.11795        0.976\n",
      "            86          -0.11713        0.976\n",
      "            87          -0.11634        0.977\n",
      "            88          -0.11556        0.977\n",
      "            89          -0.11479        0.977\n",
      "            90          -0.11404        0.977\n",
      "            91          -0.11330        0.977\n",
      "            92          -0.11257        0.977\n",
      "            93          -0.11186        0.977\n",
      "            94          -0.11116        0.977\n",
      "            95          -0.11047        0.977\n",
      "            96          -0.10979        0.977\n",
      "            97          -0.10913        0.977\n",
      "            98          -0.10847        0.977\n",
      "            99          -0.10783        0.977\n",
      "         Final          -0.10720        0.977\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.18317        0.652\n",
      "             3          -0.88250        0.755\n",
      "             4          -0.73024        0.804\n",
      "             5          -0.63301        0.832\n",
      "             6          -0.56470        0.853\n",
      "             7          -0.51364        0.867\n",
      "             8          -0.47369        0.879\n",
      "             9          -0.44135        0.889\n",
      "            10          -0.41448        0.897\n",
      "            11          -0.39167        0.904\n",
      "            12          -0.37199        0.911\n",
      "            13          -0.35478        0.916\n",
      "            14          -0.33956        0.920\n",
      "            15          -0.32598        0.923\n",
      "            16          -0.31375        0.928\n",
      "            17          -0.30268        0.933\n",
      "            18          -0.29258        0.937\n",
      "            19          -0.28333        0.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            20          -0.27482        0.943\n",
      "            21          -0.26695        0.945\n",
      "            22          -0.25964        0.946\n",
      "            23          -0.25284        0.948\n",
      "            24          -0.24649        0.950\n",
      "            25          -0.24054        0.952\n",
      "            26          -0.23496        0.953\n",
      "            27          -0.22970        0.953\n",
      "            28          -0.22475        0.955\n",
      "            29          -0.22006        0.955\n",
      "            30          -0.21562        0.956\n",
      "            31          -0.21142        0.957\n",
      "            32          -0.20742        0.958\n",
      "            33          -0.20361        0.959\n",
      "            34          -0.19999        0.959\n",
      "            35          -0.19653        0.961\n",
      "            36          -0.19322        0.962\n",
      "            37          -0.19006        0.962\n",
      "            38          -0.18703        0.963\n",
      "            39          -0.18413        0.963\n",
      "            40          -0.18134        0.964\n",
      "            41          -0.17866        0.964\n",
      "            42          -0.17609        0.965\n",
      "            43          -0.17361        0.965\n",
      "            44          -0.17122        0.966\n",
      "            45          -0.16892        0.967\n",
      "            46          -0.16670        0.968\n",
      "            47          -0.16456        0.968\n",
      "            48          -0.16248        0.968\n",
      "            49          -0.16048        0.969\n",
      "            50          -0.15854        0.969\n",
      "            51          -0.15667        0.970\n",
      "            52          -0.15485        0.970\n",
      "            53          -0.15309        0.971\n",
      "            54          -0.15138        0.971\n",
      "            55          -0.14972        0.971\n",
      "            56          -0.14811        0.972\n",
      "            57          -0.14654        0.972\n",
      "            58          -0.14502        0.972\n",
      "            59          -0.14354        0.972\n",
      "            60          -0.14211        0.972\n",
      "            61          -0.14070        0.973\n",
      "            62          -0.13934        0.973\n",
      "            63          -0.13801        0.973\n",
      "            64          -0.13672        0.973\n",
      "            65          -0.13546        0.974\n",
      "            66          -0.13423        0.974\n",
      "            67          -0.13303        0.974\n",
      "            68          -0.13186        0.974\n",
      "            69          -0.13071        0.974\n",
      "            70          -0.12959        0.974\n",
      "            71          -0.12850        0.974\n",
      "            72          -0.12744        0.974\n",
      "            73          -0.12639        0.975\n",
      "            74          -0.12537        0.975\n",
      "            75          -0.12438        0.975\n",
      "            76          -0.12340        0.975\n",
      "            77          -0.12244        0.976\n",
      "            78          -0.12151        0.976\n",
      "            79          -0.12059        0.976\n",
      "            80          -0.11970        0.976\n",
      "            81          -0.11882        0.976\n",
      "            82          -0.11795        0.976\n",
      "            83          -0.11711        0.976\n",
      "            84          -0.11628        0.976\n",
      "            85          -0.11547        0.976\n",
      "            86          -0.11467        0.976\n",
      "            87          -0.11389        0.977\n",
      "            88          -0.11312        0.977\n",
      "            89          -0.11237        0.977\n",
      "            90          -0.11163        0.977\n",
      "            91          -0.11090        0.977\n",
      "            92          -0.11019        0.977\n",
      "            93          -0.10949        0.977\n",
      "            94          -0.10880        0.977\n",
      "            95          -0.10812        0.977\n",
      "            96          -0.10746        0.977\n",
      "            97          -0.10680        0.978\n",
      "            98          -0.10616        0.978\n",
      "            99          -0.10553        0.978\n",
      "         Final          -0.10491        0.978\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19173        0.651\n",
      "             3          -0.88395        0.756\n",
      "             4          -0.73053        0.804\n",
      "             5          -0.63291        0.833\n",
      "             6          -0.56446        0.852\n",
      "             7          -0.51336        0.865\n",
      "             8          -0.47343        0.878\n",
      "             9          -0.44113        0.887\n",
      "            10          -0.41429        0.897\n",
      "            11          -0.39152        0.903\n",
      "            12          -0.37188        0.909\n",
      "            13          -0.35471        0.915\n",
      "            14          -0.33952        0.920\n",
      "            15          -0.32596        0.925\n",
      "            16          -0.31376        0.929\n",
      "            17          -0.30270        0.931\n",
      "            18          -0.29262        0.934\n",
      "            19          -0.28339        0.937\n",
      "            20          -0.27488        0.940\n",
      "            21          -0.26702        0.943\n",
      "            22          -0.25973        0.945\n",
      "            23          -0.25294        0.948\n",
      "            24          -0.24660        0.951\n",
      "            25          -0.24066        0.952\n",
      "            26          -0.23508        0.952\n",
      "            27          -0.22983        0.953\n",
      "            28          -0.22489        0.954\n",
      "            29          -0.22021        0.956\n",
      "            30          -0.21578        0.957\n",
      "            31          -0.21158        0.958\n",
      "            32          -0.20759        0.958\n",
      "            33          -0.20379        0.959\n",
      "            34          -0.20017        0.960\n",
      "            35          -0.19671        0.961\n",
      "            36          -0.19342        0.961\n",
      "            37          -0.19026        0.961\n",
      "            38          -0.18724        0.962\n",
      "            39          -0.18434        0.963\n",
      "            40          -0.18155        0.964\n",
      "            41          -0.17888        0.965\n",
      "            42          -0.17631        0.965\n",
      "            43          -0.17384        0.965\n",
      "            44          -0.17145        0.966\n",
      "            45          -0.16916        0.966\n",
      "            46          -0.16694        0.967\n",
      "            47          -0.16480        0.968\n",
      "            48          -0.16273        0.968\n",
      "            49          -0.16073        0.968\n",
      "            50          -0.15880        0.969\n",
      "            51          -0.15692        0.969\n",
      "            52          -0.15511        0.969\n",
      "            53          -0.15335        0.970\n",
      "            54          -0.15164        0.970\n",
      "            55          -0.14998        0.970\n",
      "            56          -0.14837        0.971\n",
      "            57          -0.14681        0.971\n",
      "            58          -0.14529        0.971\n",
      "            59          -0.14382        0.971\n",
      "            60          -0.14238        0.972\n",
      "            61          -0.14098        0.972\n",
      "            62          -0.13962        0.972\n",
      "            63          -0.13829        0.972\n",
      "            64          -0.13700        0.972\n",
      "            65          -0.13574        0.973\n",
      "            66          -0.13451        0.973\n",
      "            67          -0.13331        0.973\n",
      "            68          -0.13214        0.974\n",
      "            69          -0.13100        0.974\n",
      "            70          -0.12989        0.974\n",
      "            71          -0.12880        0.974\n",
      "            72          -0.12773        0.974\n",
      "            73          -0.12669        0.974\n",
      "            74          -0.12567        0.975\n",
      "            75          -0.12467        0.975\n",
      "            76          -0.12370        0.975\n",
      "            77          -0.12274        0.975\n",
      "            78          -0.12181        0.975\n",
      "            79          -0.12090        0.976\n",
      "            80          -0.12000        0.976\n",
      "            81          -0.11912        0.976\n",
      "            82          -0.11826        0.976\n",
      "            83          -0.11742        0.976\n",
      "            84          -0.11659        0.976\n",
      "            85          -0.11578        0.977\n",
      "            86          -0.11498        0.977\n",
      "            87          -0.11420        0.977\n",
      "            88          -0.11344        0.977\n",
      "            89          -0.11269        0.977\n",
      "            90          -0.11195        0.977\n",
      "            91          -0.11122        0.977\n",
      "            92          -0.11051        0.978\n",
      "            93          -0.10981        0.978\n",
      "            94          -0.10912        0.978\n",
      "            95          -0.10845        0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            96          -0.10778        0.978\n",
      "            97          -0.10713        0.978\n",
      "            98          -0.10649        0.978\n",
      "            99          -0.10586        0.978\n",
      "         Final          -0.10524        0.979\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19830        0.650\n",
      "             3          -0.88911        0.755\n",
      "             4          -0.73451        0.803\n",
      "             5          -0.63606        0.832\n",
      "             6          -0.56702        0.853\n",
      "             7          -0.51550        0.866\n",
      "             8          -0.47526        0.879\n",
      "             9          -0.44273        0.888\n",
      "            10          -0.41571        0.897\n",
      "            11          -0.39280        0.904\n",
      "            12          -0.37304        0.910\n",
      "            13          -0.35577        0.914\n",
      "            14          -0.34050        0.918\n",
      "            15          -0.32687        0.923\n",
      "            16          -0.31461        0.929\n",
      "            17          -0.30351        0.930\n",
      "            18          -0.29338        0.934\n",
      "            19          -0.28411        0.937\n",
      "            20          -0.27558        0.942\n",
      "            21          -0.26769        0.943\n",
      "            22          -0.26038        0.945\n",
      "            23          -0.25357        0.947\n",
      "            24          -0.24721        0.949\n",
      "            25          -0.24125        0.950\n",
      "            26          -0.23567        0.951\n",
      "            27          -0.23041        0.952\n",
      "            28          -0.22545        0.952\n",
      "            29          -0.22076        0.953\n",
      "            30          -0.21633        0.955\n",
      "            31          -0.21212        0.956\n",
      "            32          -0.20812        0.957\n",
      "            33          -0.20432        0.958\n",
      "            34          -0.20070        0.958\n",
      "            35          -0.19724        0.959\n",
      "            36          -0.19394        0.961\n",
      "            37          -0.19078        0.961\n",
      "            38          -0.18776        0.962\n",
      "            39          -0.18486        0.963\n",
      "            40          -0.18207        0.964\n",
      "            41          -0.17940        0.965\n",
      "            42          -0.17683        0.965\n",
      "            43          -0.17436        0.966\n",
      "            44          -0.17197        0.966\n",
      "            45          -0.16967        0.967\n",
      "            46          -0.16746        0.968\n",
      "            47          -0.16532        0.968\n",
      "            48          -0.16325        0.968\n",
      "            49          -0.16125        0.969\n",
      "            50          -0.15932        0.969\n",
      "            51          -0.15744        0.969\n",
      "            52          -0.15563        0.970\n",
      "            53          -0.15387        0.970\n",
      "            54          -0.15216        0.970\n",
      "            55          -0.15051        0.970\n",
      "            56          -0.14890        0.971\n",
      "            57          -0.14734        0.971\n",
      "            58          -0.14582        0.971\n",
      "            59          -0.14435        0.971\n",
      "            60          -0.14291        0.972\n",
      "            61          -0.14152        0.972\n",
      "            62          -0.14016        0.973\n",
      "            63          -0.13883        0.973\n",
      "            64          -0.13754        0.973\n",
      "            65          -0.13628        0.973\n",
      "            66          -0.13505        0.973\n",
      "            67          -0.13385        0.973\n",
      "            68          -0.13268        0.973\n",
      "            69          -0.13154        0.973\n",
      "            70          -0.13043        0.974\n",
      "            71          -0.12934        0.974\n",
      "            72          -0.12828        0.974\n",
      "            73          -0.12723        0.974\n",
      "            74          -0.12622        0.974\n",
      "            75          -0.12522        0.974\n",
      "            76          -0.12425        0.974\n",
      "            77          -0.12329        0.975\n",
      "            78          -0.12236        0.975\n",
      "            79          -0.12145        0.975\n",
      "            80          -0.12055        0.975\n",
      "            81          -0.11968        0.975\n",
      "            82          -0.11882        0.975\n",
      "            83          -0.11797        0.975\n",
      "            84          -0.11715        0.976\n",
      "            85          -0.11634        0.976\n",
      "            86          -0.11554        0.976\n",
      "            87          -0.11476        0.976\n",
      "            88          -0.11400        0.976\n",
      "            89          -0.11325        0.976\n",
      "            90          -0.11251        0.976\n",
      "            91          -0.11179        0.977\n",
      "            92          -0.11107        0.977\n",
      "            93          -0.11038        0.977\n",
      "            94          -0.10969        0.977\n",
      "            95          -0.10901        0.977\n",
      "            96          -0.10835        0.978\n",
      "            97          -0.10770        0.978\n",
      "            98          -0.10706        0.978\n",
      "            99          -0.10643        0.978\n",
      "         Final          -0.10581        0.978\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19066        0.650\n",
      "             3          -0.88567        0.754\n",
      "             4          -0.73232        0.804\n",
      "             5          -0.63451        0.833\n",
      "             6          -0.56587        0.851\n",
      "             7          -0.51461        0.866\n",
      "             8          -0.47455        0.879\n",
      "             9          -0.44215        0.886\n",
      "            10          -0.41524        0.896\n",
      "            11          -0.39241        0.904\n",
      "            12          -0.37271        0.911\n",
      "            13          -0.35550        0.915\n",
      "            14          -0.34027        0.919\n",
      "            15          -0.32667        0.925\n",
      "            16          -0.31444        0.928\n",
      "            17          -0.30336        0.931\n",
      "            18          -0.29325        0.936\n",
      "            19          -0.28399        0.941\n",
      "            20          -0.27546        0.944\n",
      "            21          -0.26757        0.946\n",
      "            22          -0.26025        0.947\n",
      "            23          -0.25344        0.948\n",
      "            24          -0.24707        0.949\n",
      "            25          -0.24111        0.949\n",
      "            26          -0.23550        0.952\n",
      "            27          -0.23023        0.954\n",
      "            28          -0.22526        0.954\n",
      "            29          -0.22056        0.956\n",
      "            30          -0.21611        0.957\n",
      "            31          -0.21189        0.958\n",
      "            32          -0.20787        0.958\n",
      "            33          -0.20406        0.959\n",
      "            34          -0.20042        0.959\n",
      "            35          -0.19695        0.960\n",
      "            36          -0.19363        0.961\n",
      "            37          -0.19045        0.961\n",
      "            38          -0.18741        0.962\n",
      "            39          -0.18450        0.963\n",
      "            40          -0.18170        0.964\n",
      "            41          -0.17901        0.964\n",
      "            42          -0.17643        0.965\n",
      "            43          -0.17394        0.965\n",
      "            44          -0.17155        0.966\n",
      "            45          -0.16924        0.966\n",
      "            46          -0.16701        0.967\n",
      "            47          -0.16486        0.967\n",
      "            48          -0.16278        0.968\n",
      "            49          -0.16077        0.969\n",
      "            50          -0.15882        0.970\n",
      "            51          -0.15694        0.970\n",
      "            52          -0.15511        0.971\n",
      "            53          -0.15334        0.971\n",
      "            54          -0.15163        0.971\n",
      "            55          -0.14997        0.972\n",
      "            56          -0.14835        0.972\n",
      "            57          -0.14678        0.972\n",
      "            58          -0.14525        0.972\n",
      "            59          -0.14377        0.973\n",
      "            60          -0.14233        0.973\n",
      "            61          -0.14092        0.974\n",
      "            62          -0.13955        0.974\n",
      "            63          -0.13822        0.974\n",
      "            64          -0.13692        0.974\n",
      "            65          -0.13566        0.974\n",
      "            66          -0.13442        0.975\n",
      "            67          -0.13322        0.975\n",
      "            68          -0.13204        0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            69          -0.13089        0.975\n",
      "            70          -0.12977        0.975\n",
      "            71          -0.12868        0.975\n",
      "            72          -0.12761        0.975\n",
      "            73          -0.12656        0.976\n",
      "            74          -0.12554        0.976\n",
      "            75          -0.12454        0.976\n",
      "            76          -0.12356        0.976\n",
      "            77          -0.12260        0.976\n",
      "            78          -0.12166        0.976\n",
      "            79          -0.12075        0.976\n",
      "            80          -0.11985        0.977\n",
      "            81          -0.11896        0.977\n",
      "            82          -0.11810        0.977\n",
      "            83          -0.11725        0.977\n",
      "            84          -0.11642        0.977\n",
      "            85          -0.11561        0.977\n",
      "            86          -0.11481        0.977\n",
      "            87          -0.11403        0.978\n",
      "            88          -0.11326        0.978\n",
      "            89          -0.11250        0.978\n",
      "            90          -0.11176        0.978\n",
      "            91          -0.11103        0.978\n",
      "            92          -0.11032        0.978\n",
      "            93          -0.10961        0.978\n",
      "            94          -0.10892        0.978\n",
      "            95          -0.10825        0.978\n",
      "            96          -0.10758        0.979\n",
      "            97          -0.10693        0.979\n",
      "            98          -0.10628        0.979\n",
      "            99          -0.10565        0.979\n",
      "         Final          -0.10502        0.979\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.052\n",
      "             2          -1.19917        0.650\n",
      "             3          -0.89081        0.755\n",
      "             4          -0.73643        0.801\n",
      "             5          -0.63801        0.830\n",
      "             6          -0.56899        0.850\n",
      "             7          -0.51751        0.864\n",
      "             8          -0.47732        0.876\n",
      "             9          -0.44483        0.884\n",
      "            10          -0.41786        0.893\n",
      "            11          -0.39500        0.900\n",
      "            12          -0.37529        0.906\n",
      "            13          -0.35806        0.912\n",
      "            14          -0.34282        0.917\n",
      "            15          -0.32922        0.922\n",
      "            16          -0.31699        0.926\n",
      "            17          -0.30590        0.930\n",
      "            18          -0.29580        0.933\n",
      "            19          -0.28654        0.936\n",
      "            20          -0.27802        0.939\n",
      "            21          -0.27014        0.942\n",
      "            22          -0.26283        0.943\n",
      "            23          -0.25603        0.945\n",
      "            24          -0.24967        0.946\n",
      "            25          -0.24372        0.948\n",
      "            26          -0.23813        0.951\n",
      "            27          -0.23288        0.952\n",
      "            28          -0.22792        0.953\n",
      "            29          -0.22323        0.953\n",
      "            30          -0.21879        0.955\n",
      "            31          -0.21458        0.956\n",
      "            32          -0.21058        0.956\n",
      "            33          -0.20678        0.957\n",
      "            34          -0.20315        0.958\n",
      "            35          -0.19969        0.958\n",
      "            36          -0.19638        0.958\n",
      "            37          -0.19322        0.961\n",
      "            38          -0.19019        0.961\n",
      "            39          -0.18729        0.961\n",
      "            40          -0.18450        0.962\n",
      "            41          -0.18182        0.962\n",
      "            42          -0.17924        0.962\n",
      "            43          -0.17676        0.963\n",
      "            44          -0.17437        0.963\n",
      "            45          -0.17207        0.964\n",
      "            46          -0.16985        0.964\n",
      "            47          -0.16770        0.965\n",
      "            48          -0.16563        0.965\n",
      "            49          -0.16362        0.965\n",
      "            50          -0.16168        0.966\n",
      "            51          -0.15981        0.967\n",
      "            52          -0.15799        0.967\n",
      "            53          -0.15622        0.968\n",
      "            54          -0.15451        0.968\n",
      "            55          -0.15285        0.968\n",
      "            56          -0.15124        0.968\n",
      "            57          -0.14967        0.969\n",
      "            58          -0.14815        0.969\n",
      "            59          -0.14666        0.969\n",
      "            60          -0.14522        0.970\n",
      "            61          -0.14382        0.970\n",
      "            62          -0.14245        0.970\n",
      "            63          -0.14112        0.970\n",
      "            64          -0.13983        0.971\n",
      "            65          -0.13856        0.972\n",
      "            66          -0.13733        0.972\n",
      "            67          -0.13613        0.972\n",
      "            68          -0.13495        0.972\n",
      "            69          -0.13380        0.972\n",
      "            70          -0.13268        0.972\n",
      "            71          -0.13159        0.972\n",
      "            72          -0.13052        0.973\n",
      "            73          -0.12948        0.973\n",
      "            74          -0.12845        0.973\n",
      "            75          -0.12745        0.974\n",
      "            76          -0.12647        0.974\n",
      "            77          -0.12551        0.974\n",
      "            78          -0.12458        0.974\n",
      "            79          -0.12366        0.974\n",
      "            80          -0.12276        0.974\n",
      "            81          -0.12188        0.974\n",
      "            82          -0.12101        0.974\n",
      "            83          -0.12016        0.974\n",
      "            84          -0.11933        0.974\n",
      "            85          -0.11852        0.975\n",
      "            86          -0.11772        0.975\n",
      "            87          -0.11693        0.975\n",
      "            88          -0.11616        0.975\n",
      "            89          -0.11541        0.975\n",
      "            90          -0.11466        0.975\n",
      "            91          -0.11394        0.975\n",
      "            92          -0.11322        0.975\n",
      "            93          -0.11252        0.976\n",
      "            94          -0.11183        0.976\n",
      "            95          -0.11115        0.976\n",
      "            96          -0.11048        0.976\n",
      "            97          -0.10982        0.976\n",
      "            98          -0.10918        0.976\n",
      "            99          -0.10854        0.977\n",
      "         Final          -0.10792        0.977\n",
      "Overall accuracy of the model 0.8142371444374014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>className</th>\n",
       "      <th>avgPrecision</th>\n",
       "      <th>avgRecall</th>\n",
       "      <th>avgF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Statement</td>\n",
       "      <td>0.735113</td>\n",
       "      <td>0.871889</td>\n",
       "      <td>0.797396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Emotion</td>\n",
       "      <td>0.831115</td>\n",
       "      <td>0.796536</td>\n",
       "      <td>0.812683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>System</td>\n",
       "      <td>0.964643</td>\n",
       "      <td>0.964670</td>\n",
       "      <td>0.964545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Greet</td>\n",
       "      <td>0.915193</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>0.917728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Accept</td>\n",
       "      <td>0.550609</td>\n",
       "      <td>0.416123</td>\n",
       "      <td>0.466742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Reject</td>\n",
       "      <td>0.297410</td>\n",
       "      <td>0.164167</td>\n",
       "      <td>0.209010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>whQuestion</td>\n",
       "      <td>0.842134</td>\n",
       "      <td>0.735570</td>\n",
       "      <td>0.783681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Continuer</td>\n",
       "      <td>0.404556</td>\n",
       "      <td>0.196691</td>\n",
       "      <td>0.255594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ynQuestion</td>\n",
       "      <td>0.703419</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.669130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>yAnswer</td>\n",
       "      <td>0.410675</td>\n",
       "      <td>0.288182</td>\n",
       "      <td>0.336055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bye</td>\n",
       "      <td>0.869602</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.706801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Clarify</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Emphasis</td>\n",
       "      <td>0.541136</td>\n",
       "      <td>0.352632</td>\n",
       "      <td>0.424031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>nAnswer</td>\n",
       "      <td>0.668571</td>\n",
       "      <td>0.333929</td>\n",
       "      <td>0.427511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     className  avgPrecision  avgRecall     avgF1\n",
       "0    Statement      0.735113   0.871889  0.797396\n",
       "1      Emotion      0.831115   0.796536  0.812683\n",
       "2       System      0.964643   0.964670  0.964545\n",
       "3        Greet      0.915193   0.920792  0.917728\n",
       "4       Accept      0.550609   0.416123  0.466742\n",
       "5       Reject      0.297410   0.164167  0.209010\n",
       "6   whQuestion      0.842134   0.735570  0.783681\n",
       "7    Continuer      0.404556   0.196691  0.255594\n",
       "8   ynQuestion      0.703419   0.640000  0.669130\n",
       "9      yAnswer      0.410675   0.288182  0.336055\n",
       "10         Bye      0.869602   0.605000  0.706801\n",
       "11     Clarify      0.000000   0.000000  0.000000\n",
       "12    Emphasis      0.541136   0.352632  0.424031\n",
       "13     nAnswer      0.668571   0.333929  0.427511\n",
       "14       Other      0.650000   0.350000  0.430000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Maxnet model\n",
    "featuresets = [(feature(post.text), post.get('class'))  for post in posts]\n",
    "random.shuffle(featuresets)\n",
    "dfPre,dfRec,dfF1=model(featuresets,classList,'MXN')\n",
    "\n",
    "df=pd.merge(dfPre,dfRec,on='className',how='inner')\n",
    "df=pd.merge(df,dfF1,on='className',how='inner')\n",
    "\n",
    "df.to_csv(\"dfmaxNet.csv\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dqy3NMQo9hS1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model 0.6154760668569444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>className</th>\n",
       "      <th>avgPrecision</th>\n",
       "      <th>avgRecall</th>\n",
       "      <th>avgF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Statement</td>\n",
       "      <td>0.566133</td>\n",
       "      <td>0.691663</td>\n",
       "      <td>0.622516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Emotion</td>\n",
       "      <td>0.411109</td>\n",
       "      <td>0.508108</td>\n",
       "      <td>0.454321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>System</td>\n",
       "      <td>0.856672</td>\n",
       "      <td>0.897794</td>\n",
       "      <td>0.876657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Greet</td>\n",
       "      <td>0.493660</td>\n",
       "      <td>0.673481</td>\n",
       "      <td>0.569277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Accept</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Reject</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>whQuestion</td>\n",
       "      <td>0.890247</td>\n",
       "      <td>0.530678</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Continuer</td>\n",
       "      <td>0.538077</td>\n",
       "      <td>0.190809</td>\n",
       "      <td>0.263007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ynQuestion</td>\n",
       "      <td>0.553748</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.204690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>yAnswer</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bye</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Clarify</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Emphasis</td>\n",
       "      <td>0.463952</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.412083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>nAnswer</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     className  avgPrecision  avgRecall     avgF1\n",
       "0    Statement      0.566133   0.691663  0.622516\n",
       "1      Emotion      0.411109   0.508108  0.454321\n",
       "2       System      0.856672   0.897794  0.876657\n",
       "3        Greet      0.493660   0.673481  0.569277\n",
       "4       Accept      0.000000   0.000000  0.000000\n",
       "5       Reject      0.000000   0.000000  0.000000\n",
       "6   whQuestion      0.890247   0.530678  0.662648\n",
       "7    Continuer      0.538077   0.190809  0.263007\n",
       "8   ynQuestion      0.553748   0.127273  0.204690\n",
       "9      yAnswer      0.000000   0.000000  0.000000\n",
       "10         Bye      0.000000   0.000000  0.000000\n",
       "11     Clarify      0.000000   0.000000  0.000000\n",
       "12    Emphasis      0.463952   0.384211  0.412083\n",
       "13     nAnswer      0.000000   0.000000  0.000000\n",
       "14       Other      0.000000   0.000000  0.000000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.Decision tree model (This took very long time)\n",
    "featuresets = [(feature(post.text), post.get('class'))  for post in posts]\n",
    "random.shuffle(featuresets)\n",
    "dfPre,dfRec,dfF1=model(featuresets,classList,'DT')\n",
    "\n",
    "df=pd.merge(dfPre,dfRec,on='className',how='inner')\n",
    "df=pd.merge(df,dfF1,on='className',how='inner')\n",
    "\n",
    "df.to_csv(\"dfDT.csv\",index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3_Final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
